"""Streamlit entry point for the PPT translation prototype."""

from __future__ import annotations

import html
import io
import json
import logging
import math
import queue
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Tuple

import streamlit as st
import streamlit.components.v1 as components
from PIL import Image

from src.chains.context_manager import ContextManager
from src.chains.translation_chain import create_translation_chain, translate_with_progress
from src.core.ppt_parser import PPTParser
from src.core.ppt_writer import PPTWriter
from src.core.text_extractor import ExtractionOptions, docs_to_markdown, extract_pptx_to_docs
from src.ui.extraction_settings import render_extraction_settings
from src.ui.file_handler import get_cached_upload, handle_upload
from src.ui.progress_tracker import ProgressTracker
from src.ui.settings_panel import render_settings
from src.utils.config import get_settings
from src.utils.glossary_loader import GlossaryLoader
from src.utils.helpers import chunk_paragraphs
from src.utils.repetition import build_repetition_plan, expand_translations
from src.utils.language_detector import LanguageDetector

logging.basicConfig(level=logging.INFO, format="[%(levelname)s] %(name)s: %(message)s")
LOGGER = logging.getLogger(__name__)

CAT_IMAGE_PATH = Path(__file__).resolve().parent / "assets" / "ë²ˆì—­ìº£ íšŒìƒ‰.png"
try:
    resample = Image.Resampling.LANCZOS  # type: ignore[attr-defined]
except AttributeError:  # pragma: no cover - Pillow < 9 fallback
    resample = Image.LANCZOS

try:
    CAT_IMAGE = Image.open(CAT_IMAGE_PATH)
    CAT_IMAGE_SCALED = CAT_IMAGE.resize(
        (
            max(1, int(CAT_IMAGE.width * 0.7)),
            max(1, int(CAT_IMAGE.height * 0.7)),
        ),
        resample,
    )
except FileNotFoundError:  # pragma: no cover - asset expected to be present in prod
    CAT_IMAGE = None
    CAT_IMAGE_SCALED = None

st.set_page_config(page_title="PPT ë²ˆì—­ìº£", page_icon=CAT_IMAGE or "ğŸ“Š", layout="wide")

MAX_UI_LOG_LINES = 400
LOG_QUEUE_KEY = "ui_log_queue"
LOG_BUFFER_KEY = "ui_log_buffer"
LOG_DIRTY_KEY = "ui_log_dirty"
LOG_HANDLER_KEY = "ui_log_handler_attached"
EXTRACTION_STATE_KEY = "text_extraction_state"


class StreamlitLogHandler(logging.Handler):
    """Thread-safe log handler that enqueues messages for the UI thread."""

    def __init__(self, target_queue: "queue.SimpleQueue[str]") -> None:
        super().__init__(level=logging.INFO)
        self._queue = target_queue
        self.setFormatter(logging.Formatter("[%(levelname)s] %(name)s: %(message)s"))

    def emit(self, record: logging.LogRecord) -> None:  # noqa: D401 - see class docstring
        try:
            message = self.format(record)
        except Exception:  # pragma: no cover - defensive
            return

        try:
            self._queue.put_nowait(message)
        except queue.Full:  # pragma: no cover - queue is unbounded but defensive check
            pass


def _initialise_log_state() -> tuple[queue.SimpleQueue[str], List[str]]:
    """Ensure session state contains queue and buffer for UI logs."""

    if LOG_QUEUE_KEY not in st.session_state:
        st.session_state[LOG_QUEUE_KEY] = queue.SimpleQueue()
    if LOG_BUFFER_KEY not in st.session_state:
        st.session_state[LOG_BUFFER_KEY] = []
    if LOG_DIRTY_KEY not in st.session_state:
        st.session_state[LOG_DIRTY_KEY] = True
    st.session_state.setdefault(LOG_HANDLER_KEY, False)
    return st.session_state[LOG_QUEUE_KEY], st.session_state[LOG_BUFFER_KEY]


def _drain_log_queue(target_buffer: List[str]) -> None:
    """Transfer queued log messages into the render buffer on the main thread."""

    message_queue: queue.SimpleQueue[str] = st.session_state[LOG_QUEUE_KEY]

    while True:
        try:
            message = message_queue.get_nowait()
        except queue.Empty:
            break
        target_buffer.append(message)

    if len(target_buffer) > MAX_UI_LOG_LINES:
        del target_buffer[: len(target_buffer) - MAX_UI_LOG_LINES]

    st.session_state[LOG_DIRTY_KEY] = True


def _render_log_panel(placeholder: Any, log_buffer: List[str]) -> None:
    """Render buffered logs inside the provided placeholder."""

    if not log_buffer:
        placeholder.info("ë¡œê·¸ê°€ ì•„ì§ ì—†ìŠµë‹ˆë‹¤.")
    else:
        placeholder.markdown("```\n" + "\n".join(log_buffer) + "\n```")
    st.session_state[LOG_DIRTY_KEY] = False


def _refresh_ui_logs(placeholder: Any, log_buffer: List[str]) -> None:
    """Drain queued logs and update the Streamlit panel if required."""

    _drain_log_queue(log_buffer)
    if st.session_state.get(LOG_DIRTY_KEY):
        _render_log_panel(placeholder, log_buffer)


def _approximate_tokens(text: str) -> int:
    """Rudimentary character-based token estimate for heuristics."""

    if not text:
        return 0
    return max(1, len(text) // 4)


def _estimate_tokens_for_batch(batch: Dict[str, object]) -> int:
    """Estimate total prompt tokens for a single translation batch."""

    texts = str(batch.get("texts", ""))
    ppt_context = str(batch.get("ppt_context", ""))
    glossary_terms = str(batch.get("glossary_terms", ""))

    token_estimate = (
        _approximate_tokens(texts)
        + _approximate_tokens(ppt_context)
        + _approximate_tokens(glossary_terms)
        + 200  # instructions + response padding
    )

    return max(1, token_estimate)


def _attach_streamlit_log_handler(log_queue: "queue.SimpleQueue[str]") -> None:
    """Attach the Streamlit log handler on the root logger once per session."""

    if st.session_state.get(LOG_HANDLER_KEY):
        return

    root_logger = logging.getLogger()
    root_logger.addHandler(StreamlitLogHandler(log_queue))
    st.session_state[LOG_HANDLER_KEY] = True


def _load_glossary(glossary_file) -> Tuple[dict[str, str] | None, str]:
    """Load glossary data from the uploaded file."""

    if glossary_file is None:
        return None, "None"

    glossary_loader = GlossaryLoader()
    glossary_bytes = io.BytesIO(glossary_file.getvalue())

    try:
        glossary = glossary_loader.load_glossary(glossary_bytes)
    except ValueError as exc:
        st.error(str(exc))
        return None, "None"

    st.success(f"ğŸ“š ìš©ì–´ì§‘ ë¡œë“œ ì™„ë£Œ: {len(glossary)}ê±´")
    return glossary, GlossaryLoader.format_glossary_terms(glossary)


def _determine_batch_size(total_paragraphs: int, settings) -> int:
    """Calculate a batch size that balances latency and throughput."""

    if total_paragraphs <= 0:
        return 1

    min_size = max(1, int(getattr(settings, "min_batch_size", 40)))
    max_size = max(min_size, int(getattr(settings, "max_batch_size", getattr(settings, "batch_size", min_size))))
    default_size = max(min_size, min(max_size, int(getattr(settings, "batch_size", max_size))))

    concurrency = max(1, int(getattr(settings, "max_concurrency", 1)))
    wave_multiplier = float(getattr(settings, "wave_multiplier", 1.2) or 1.2)
    wave_multiplier = max(1.0, wave_multiplier)

    target_batches = max(concurrency, int(math.ceil(concurrency * wave_multiplier * 2)))
    suggested_size = math.ceil(total_paragraphs / target_batches) if target_batches > 0 else default_size

    batch_size = max(min_size, min(max_size, suggested_size))
    if batch_size < default_size:
        batch_size = max(batch_size, min(default_size, max_size))

    actual_batches = max(1, math.ceil(total_paragraphs / batch_size))
    if actual_batches > 1:
        remainder = total_paragraphs - (actual_batches - 1) * batch_size
        if 0 < remainder < max(1, int(min_size * 0.5)):
            adjusted = math.ceil(total_paragraphs / (actual_batches - 1))
            batch_size = max(min_size, min(max_size, adjusted))

    return max(1, min(total_paragraphs, batch_size))


def _sanitize_for_filename(value: str, fallback: str) -> str:
    """Remove characters that are risky inside file names while keeping unicode."""

    sanitized = "".join(ch for ch in value if ch.isalnum() or ch in ("-", "_"))
    return sanitized or fallback


def _get_extraction_state() -> Dict[str, Any]:
    state = st.session_state.setdefault(EXTRACTION_STATE_KEY, {})
    state.setdefault("markdown", "")
    state.setdefault("file_name", None)
    state.setdefault("options", None)
    state.setdefault("slides", 0)
    state.setdefault("blocks", 0)
    state.setdefault("stale", False)
    return state


def _render_text_extraction_page(settings, extraction_options: ExtractionOptions) -> None:
    """Render PPT text extraction workflow."""

    st.title("ğŸ§¾ PPT í…ìŠ¤íŠ¸ ì¶”ì¶œ")
    st.markdown("PPT íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì—¬ Markdown í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    uploaded_file = st.file_uploader(
        "PPTX íŒŒì¼ ì—…ë¡œë“œ",
        type=["ppt", "pptx"],
        key="text_extraction_uploader",
        help="ìµœëŒ€ %dMBê¹Œì§€ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤." % settings.max_upload_size_mb,
    )

    state = _get_extraction_state()
    current_signature = {
        "figures": extraction_options.figures,
        "charts": extraction_options.charts,
        "with_notes": extraction_options.with_notes,
    }

    if state["markdown"]:
        if uploaded_file and uploaded_file.name != state["file_name"]:
            state["stale"] = True
        elif state["options"] != current_signature:
            state["stale"] = True
        else:
            state["stale"] = False
    else:
        state["stale"] = False

    convert_clicked = st.button(
        "Markdown ë³€í™˜",
        type="primary",
        disabled=uploaded_file is None,
    )

    if convert_clicked and uploaded_file is not None:
        size_mb = uploaded_file.size / (1024 * 1024)
        if size_mb > settings.max_upload_size_mb:
            st.error(
                f"íŒŒì¼ í¬ê¸°ê°€ {settings.max_upload_size_mb}MBë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤. ë” ì‘ì€ íŒŒì¼ë¡œ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            )
        else:
            ppt_buffer = io.BytesIO(uploaded_file.getvalue())
            ppt_buffer.seek(0)
            try:
                docs = extract_pptx_to_docs(ppt_buffer, extraction_options)
            except Exception as exc:  # pylint: disable=broad-except
                LOGGER.exception("Extraction failed: %s", exc)
                st.error("í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. íŒŒì¼ì„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.")
            else:
                markdown_text = docs_to_markdown(docs, extraction_options)
                total_blocks = sum(len(doc.blocks) for doc in docs)
                state.update(
                    {
                        "markdown": markdown_text,
                        "file_name": uploaded_file.name,
                        "options": current_signature,
                        "slides": len(docs),
                        "blocks": total_blocks,
                        "stale": False,
                    }
                )
                st.session_state["markdown_preview"] = markdown_text
                if markdown_text.strip():
                    st.success(f"ì´ {len(docs)}ê°œì˜ ìŠ¬ë¼ì´ë“œì—ì„œ {total_blocks}ê°œì˜ ë¸”ë¡ì„ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.")
                else:
                    st.warning("ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

    if state["stale"]:
        st.info("ì˜µì…˜ì´ë‚˜ íŒŒì¼ì´ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ë³€í™˜ì„ ì‹¤í–‰í•˜ë©´ ìµœì‹  ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    markdown_value = state["markdown"]
    if "markdown_preview" not in st.session_state:
        st.session_state["markdown_preview"] = markdown_value

    # ë²„íŠ¼ì„ ë¨¼ì € í‘œì‹œ
    if markdown_value.strip():
        safe_name = _sanitize_for_filename(Path(state["file_name"] or "presentation").stem, "presentation")
        download_name = f"{safe_name}_extracted.md"
        
        col1, col2 = st.columns([1, 1])
        with col1:
            st.download_button(
                "ğŸ“¥ Markdown ë‹¤ìš´ë¡œë“œ",
                data=markdown_value.encode("utf-8"),
                file_name=download_name,
                mime="text/markdown",
                use_container_width=True,
            )
        with col2:
            # JavaScriptë¥¼ ì‚¬ìš©í•œ í´ë¦½ë³´ë“œ ë³µì‚¬
            # JSONìœ¼ë¡œ ì§ë ¬í™”í•˜ì—¬ ì•ˆì „í•˜ê²Œ JavaScriptë¡œ ì „ë‹¬
            escaped_markdown = json.dumps(markdown_value)
            copy_html = f"""
            <!DOCTYPE html>
            <html>
            <head>
                <style>
                    body {{
                        margin: 0;
                        padding: 0;
                        font-family: 'Source Sans Pro', sans-serif;
                    }}
                    button {{
                        width: 100%;
                        padding: 0.375rem 0.75rem;
                        background-color: rgb(255, 255, 255);
                        color: rgb(49, 51, 63);
                        border: 1px solid rgba(49, 51, 63, 0.2);
                        border-radius: 0.5rem;
                        font-family: 'Source Sans Pro', sans-serif;
                        font-size: 1rem;
                        font-weight: 400;
                        line-height: 1.6;
                        cursor: pointer;
                        transition: all 0.2s;
                    }}
                    button:hover {{
                        border-color: rgb(255, 75, 75);
                        color: rgb(255, 75, 75);
                    }}
                </style>
            </head>
            <body>
                <button onclick="copyToClipboard()">ğŸ“‹ í´ë¦½ë³´ë“œ ë³µì‚¬</button>
                <script>
                    const text = {escaped_markdown};
                    
                    function copyToClipboard() {{
                        navigator.clipboard.writeText(text).then(() => {{
                            const btn = document.querySelector('button');
                            btn.textContent = 'âœ… ë³µì‚¬ ì™„ë£Œ!';
                            setTimeout(() => {{
                                btn.textContent = 'ğŸ“‹ í´ë¦½ë³´ë“œ ë³µì‚¬';
                            }}, 2000);
                        }}).catch(err => {{
                            alert('ë³µì‚¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: ' + err);
                        }});
                    }}
                </script>
            </body>
            </html>
            """
            components.html(copy_html, height=50)

    # ë¯¸ë¦¬ë³´ê¸°ë¥¼ ë²„íŠ¼ ì•„ë˜ì— í‘œì‹œ
    st.subheader("Markdown ë¯¸ë¦¬ë³´ê¸°")
    st.code(
        st.session_state.get("markdown_preview", ""),
        language="markdown",
        line_numbers=False,
    )



def _render_translation_page(settings, settings_state: Dict[str, Any]) -> None:
    """Render PPT translation workflow."""

    st.title("ğŸŒ ë²ˆì—­ëœ PPT ìƒì„±")
    st.markdown("ì›ë³¸ PPTì˜ ë””ìì¸ì„ ìœ ì§€í•˜ë©´ì„œ ë‚´ë¶€ í…ìŠ¤íŠ¸ë§Œ ë²ˆì—­í•œ ìƒˆ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.")

    if not settings.openai_api_key:
        st.warning("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë²ˆì—­ ì‹¤í–‰ ì‹œ ì—ëŸ¬ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    preprocess_repetitions = bool(settings_state.get("preprocess_repetitions"))
    if preprocess_repetitions:
        st.info("ë°˜ë³µ ë¬¸êµ¬ ì‚¬ì „ ì²˜ë¦¬ ì˜µì…˜ì´ í™œì„±í™”ë˜ì–´ ë™ì¼ ë¬¸ì¥ì„ í•œ ë²ˆë§Œ ë²ˆì—­í•©ë‹ˆë‹¤.")

    log_panel = st.expander("ğŸ“œ ì‹¤í–‰ ë¡œê·¸", expanded=True)
    log_placeholder = log_panel.empty()
    log_queue, log_buffer = _initialise_log_state()
    _attach_streamlit_log_handler(log_queue)
    _refresh_ui_logs(log_placeholder, log_buffer)

    uploaded_file = st.file_uploader(
        "PPT íŒŒì¼ ì—…ë¡œë“œ",
        type=["ppt", "pptx"],
        key="translation_uploader",
        help="ìµœëŒ€ %dMBê¹Œì§€ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤." % settings.max_upload_size_mb,
    )

    ppt_buffer = None
    if uploaded_file:
        ppt_buffer = handle_upload(uploaded_file, max_size_mb=settings.max_upload_size_mb)
        _refresh_ui_logs(log_placeholder, log_buffer)

    if not ppt_buffer:
        return

    if st.button("ğŸš€ ë²ˆì—­ ì‹œì‘", type="primary"):
        with st.spinner("ë²ˆì—­ ì§„í–‰ ì¤‘..."):
            log_buffer.clear()
            while True:
                try:
                    log_queue.get_nowait()
                except queue.Empty:
                    break
            st.session_state[LOG_DIRTY_KEY] = True
            if st.session_state.get(LOG_HANDLER_KEY, False) is False:
                _attach_streamlit_log_handler(log_queue)
            _render_log_panel(log_placeholder, log_buffer)

            cached_buffer = get_cached_upload()
            if cached_buffer is None:
                st.error("ì—…ë¡œë“œëœ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
                return

            parser = PPTParser()
            paragraphs, presentation = parser.extract_paragraphs(cached_buffer)
            _refresh_ui_logs(log_placeholder, log_buffer)

            if not paragraphs:
                st.warning("ë²ˆì—­í•  í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return

            if len(presentation.slides) > 100:
                st.warning("ìŠ¬ë¼ì´ë“œê°€ 100ì¥ì„ ì´ˆê³¼í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì‹œê°„ì´ ê¸¸ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

            context_manager = ContextManager(paragraphs)
            ppt_context = context_manager.build_global_context()

            glossary, glossary_terms = _load_glossary(settings_state.get("glossary_file"))
            prepared_texts: List[str] = [info.original_text for info in paragraphs]
            if glossary:
                prepared_texts = GlossaryLoader.apply_glossary_to_texts(prepared_texts, glossary)

            repetition_plan = None
            target_paragraphs = paragraphs
            target_prepared_texts = prepared_texts

            if preprocess_repetitions:
                repetition_plan = build_repetition_plan(paragraphs)
                target_paragraphs = [paragraphs[idx] for idx in repetition_plan.unique_indices]
                target_prepared_texts = [prepared_texts[idx] for idx in repetition_plan.unique_indices]

                duplicates_info = repetition_plan.duplicate_counts()
                reduced = len(paragraphs) - len(target_paragraphs)
                if duplicates_info:
                    st.caption(
                        f"ë°˜ë³µ ë¬¸êµ¬ {len(duplicates_info)}ê°œ ê°ì§€: ë²ˆì—­ ë¬¸ì¥ ìˆ˜ {len(paragraphs)} â†’ {len(target_paragraphs)} (ê°ì†Œ {reduced})"
                    )
                    preview = sorted(duplicates_info.items(), key=lambda item: item[1], reverse=True)
                    with st.expander("ë°˜ë³µ ë¬¸êµ¬ ë¯¸ë¦¬ë³´ê¸°", expanded=False):
                        rows = "<br>".join(
                            f"<strong>{count}Ã—</strong>: {html.escape(text)}"
                            for text, count in preview
                        )
                        st.markdown(
                            """
                            <div style="max-height: 280px; overflow-y: auto; padding-right: 6px;">
                                {rows}
                            </div>
                            """.format(rows=rows or "<em>ì¤‘ë³µ ë¬¸ì¥ì´ ì—†ìŠµë‹ˆë‹¤.</em>"),
                            unsafe_allow_html=True,
                        )
                else:
                    st.caption("ë°˜ë³µ ë¬¸êµ¬ ì‚¬ì „ ì²˜ë¦¬ ê²°ê³¼ ì¤‘ë³µ ë¬¸ì¥ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

                if not target_paragraphs:
                    st.warning("ë°˜ë³µ ë¬¸êµ¬ ì‚¬ì „ ì²˜ë¦¬ ê²°ê³¼ ë²ˆì—­í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    return

            detector = LanguageDetector()
            sample_text = "\n".join(paragraph.original_text for paragraph in paragraphs[:50])

            source_language = settings_state.get("source_lang")
            target_language = settings_state.get("target_lang")

            if source_language == "Auto":
                source_language = detector.detect_language(sample_text)
                st.info(f"ğŸ” ì†ŒìŠ¤ ì–¸ì–´ ê°ì§€: {source_language}")

            if target_language == "Auto":
                target_language = detector.infer_target_language(source_language)
                st.info(f"ğŸ” íƒ€ê²Ÿ ì–¸ì–´ ì¶”ë¡ : {target_language}")

            batch_size = _determine_batch_size(len(target_paragraphs), settings)

            batches = chunk_paragraphs(
                target_paragraphs,
                batch_size=batch_size,
                ppt_context=ppt_context,
                glossary_terms=glossary_terms,
                prepared_texts=target_prepared_texts,
            )

            LOGGER.info(
                "Prepared %d batches (batch size %d, unique paragraphs %d of %d total).",
                len(batches),
                batch_size,
                len(target_paragraphs),
                len(paragraphs),
            )
            _refresh_ui_logs(log_placeholder, log_buffer)

            if not batches:
                st.warning("ë²ˆì—­í•  ë°°ì¹˜ë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                return

            estimated_tokens = _estimate_tokens_for_batch(batches[0])
            safe_concurrency = max(
                1,
                min(
                    int(settings.max_concurrency),
                    max(1, settings.tpm_limit // max(estimated_tokens, 1)),
                ),
            )

            LOGGER.info(
                "Estimated %d tokens per batch; using concurrency=%d (config max=%d, TPM limit=%d).",
                estimated_tokens,
                safe_concurrency,
                settings.max_concurrency,
                settings.tpm_limit,
            )
            _refresh_ui_logs(log_placeholder, log_buffer)

            st.caption(
                f"ë°°ì¹˜ í¬ê¸°: {batch_size} ë¬¸ì¥ (ê³ ìœ  {len(target_paragraphs)} / ì „ì²´ {len(paragraphs)}) | ìµœëŒ€ ë™ì‹œ ì‹¤í–‰: {safe_concurrency}"
            )

            progress_tracker = ProgressTracker(
                total_batches=len(batches),
                total_sentences=len(target_paragraphs),
                log_update_fn=lambda: _refresh_ui_logs(log_placeholder, log_buffer),
            )

            chain = create_translation_chain(
                model_name=settings_state.get("model", "gpt-5"),
                source_lang=source_language,
                target_lang=target_language,
                user_prompt=settings_state.get("user_prompt"),
            )

            try:
                LOGGER.info(
                    "Starting translation with concurrency=%d and model=%s.",
                    safe_concurrency,
                    settings_state.get("model", "gpt-5"),
                )
                _refresh_ui_logs(log_placeholder, log_buffer)
                translated_unique = translate_with_progress(
                    chain,
                    batches,
                    progress_tracker,
                    max_concurrency=safe_concurrency,
                )
            except Exception as exc:  # pylint: disable=broad-except
                LOGGER.exception("Translation failed: %s", exc)
                st.error("ë²ˆì—­ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                return

            if repetition_plan is not None:
                translated_texts = expand_translations(
                    repetition_plan,
                    translated_unique,
                    len(paragraphs),
                )
            else:
                translated_texts = translated_unique

            if glossary:
                translated_texts = [
                    GlossaryLoader.apply_glossary_to_translation(text, glossary)
                    for text in translated_texts
                ]

            writer = PPTWriter()
            output_buffer = writer.apply_translations(paragraphs, translated_texts, presentation)
            _refresh_ui_logs(log_placeholder, log_buffer)

            total_elapsed = progress_tracker.finish()
            minutes, seconds = divmod(total_elapsed, 60)
            LOGGER.info("Translation completed in %dë¶„ %.1fì´ˆ", int(minutes), seconds)
            _refresh_ui_logs(log_placeholder, log_buffer)
            st.success(f"âœ… ë²ˆì—­ ì™„ë£Œ! ì´ ì†Œìš” ì‹œê°„: {int(minutes)}ë¶„ {seconds:.1f}ì´ˆ")

            original_name = st.session_state.get("uploaded_ppt_name", "presentation")
            original_stem = Path(original_name).stem or "presentation"
            original_stem = _sanitize_for_filename(original_stem, "presentation")
            clean_model = _sanitize_for_filename(settings_state.get("model", "model"), "model")
            timestamp = datetime.now().strftime("%Y%m%d")
            safe_target_lang = _sanitize_for_filename(target_language, "target")
            download_name = f"{safe_target_lang}_{original_stem}_{clean_model}_{timestamp}.pptx"

            st.download_button(
                label="ğŸ“¥ ë²ˆì—­ëœ PPT ë‹¤ìš´ë¡œë“œ",
                data=output_buffer.getvalue(),
                file_name=download_name,
                mime="application/vnd.openxmlformats-officedocument.presentationml.presentation",
            )


def main() -> None:
    """Render the Streamlit UI and orchestrate workflows."""

    settings = get_settings()

    st.sidebar.markdown(
        """
        <div style="text-align: center; font-size: 2rem; font-weight: 700; margin-bottom: 0.5rem;">
            PPT ë²ˆì—­ìº£
        </div>
        """,
        unsafe_allow_html=True,
    )

    if CAT_IMAGE_SCALED is not None:
        st.sidebar.image(CAT_IMAGE_SCALED)
    elif CAT_IMAGE is not None:
        st.sidebar.image(CAT_IMAGE)

    st.sidebar.markdown("### ê¸°ëŠ¥ ì„ íƒ")
    feature = st.sidebar.radio(
        "ê¸°ëŠ¥ ì„ íƒ",
        options=("PPT ë²ˆì—­", "í…ìŠ¤íŠ¸ ì¶”ì¶œ"),
        index=0,
        label_visibility="collapsed",
    )

    if feature == "í…ìŠ¤íŠ¸ ì¶”ì¶œ":
        extraction_options = render_extraction_settings(st.sidebar)
        _render_text_extraction_page(settings, extraction_options)
    else:
        translation_settings = render_settings(st.sidebar)
        _render_translation_page(settings, translation_settings)


if __name__ == "__main__":
    main()
